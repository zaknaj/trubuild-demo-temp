"""
TruBuild - Technical Report Generator

This module builds the final technical-evaluation report JSON that goes back
to the front-end.  It consumes two pre-existing artefacts:

1. RFP result JSON - the huge structure produced by the 'tender-evaluation'
   pipeline (`result.tenderReport` with `totalScore`, etc.).
2. Evaluation criteria JSON - the weighted rubric generated by the
   Tech-criteria generator (`{"Theme": {"criterion": 10, ...}, ...}`).

The workflow:
    1. Ask LLM for a short executive summary of the entire evaluation.
    2. For each contractor block, ask Gemini for a 4-sentence summary.
    3. Compute theme weights straight from the criteria JSON.
    4. Return a single dict that matches the old shape but is reproducible and
       fully logged.

Retries, token counting, and rate limiting are handled by the shared
``utils.LLM.call_llm_sync`` helper.
"""

from __future__ import annotations

import re
import json
from typing import Any, Dict, List
from google.genai.types import Part
from utils.llm.LLM import call_llm_async
from utils.core.log import pid_tool_logger, set_logger, get_logger
from tools.tech_rfp.job_queue_integration import get_latest_completed_artifact

_MODEL = "gemini-2.5-flash"
_CFG: dict[str, Any] = {
    "temperature": 0.2,
    "max_output_tokens": 20000,
    "thinking_config": {"thinking_budget": 0},
    "top_p": 1.0,
    "top_k": 1,
    "seed": 1234,
    "response_mime_type": "text/plain",
}

def _exec_summary_prompt(tender_report_block: Any) -> str:
    return (
        "Summarise the following tenderReport in no more than four sentences. "
        "Be factual, concise, and avoid marketing language.\n\n"
        "```json\n"
        f"{json.dumps(tender_report_block, indent=2)}\n"
        "```"
    )


def _contractor_prompt(contractor_block: dict) -> str:
    return (
        "Summarise the contractor's submission and evaluation in four sentences "
        "or less. Focus on notable strengths, weaknesses, and overall score.\n\n"
        "```json\n"
        f"{json.dumps(contractor_block, indent=2)}\n"
        "```"
    )


# LLM wrapper
async def _ask_llm(prompt: str) -> str:
    """
    One-shot call to Gemini via call_llm_sync.
    """
    return await call_llm_async(
        prompt_parts=[Part.from_text(text=prompt)],
        model=_MODEL,
        cfg=_CFG,
        system_instruction="You are TruBuild, a professional tender analyst.",
    )


def _coerce_weight_to_float(val) -> float:
    """
    Accepts raw weight which may be:
      - number (int/float)
      - string ("15%", "0.15", " 40 ")
      - dict with {"weight": <number|string>, ...}
    Returns a float (0.0 if not parseable).
    """
    # unwrap dicts of the form {"weight": ...}
    if isinstance(val, dict):
        val = val.get("weight")

    if val is None:
        return 0.0

    if isinstance(val, (int, float)):
        try:
            return float(val)
        except Exception:
            return 0.0

    if isinstance(val, str):
        s = val.strip().replace(",", ".")
        # keep digits, minus and dot so "-10.5" still works
        s = re.sub(r"[^0-9.\-]", "", s)
        try:
            return float(s) if s else 0.0
        except Exception:
            return 0.0

    return 0.0


# public entrypoint
async def build_technical_report(
    *,
    rfp_result_data: dict,
    evaluation_criteria: Dict[str, Dict[str, float]],
) -> Dict[str, Any]:
    """
    Construct the final technical-report JSON.

    Parameters
    ----------
    rfp_result_data : dict
        The parsed RFP evaluation result JSON.
    evaluation_criteria : dict
        Weighted rubric {theme: {criterion: weight}}

    Returns
    -------
    dict
        {
          "executive_summary": "...",
          "contractorEvaluations": [
              {"name": "...", "total_score": 88.2, "evaluationSummary": "..."},
              ...
          ],
          "evaluationScopes": {
              "summary": "...",
              "scopes": [{"name": theme, "weight": totalWeight}, ...]
          }
        }
    """
    logger = get_logger()
    # load JSON if path provided

    tender_report: List[dict] = rfp_result_data["result"]["tenderReport"]

    # executive summary
    exec_summary = await _ask_llm(_exec_summary_prompt(tender_report))

    # per-contractor summaries
    contractor_evals: List[Dict[str, Any]] = []
    for bloc in tender_report:
        summary = await _ask_llm(_contractor_prompt(bloc))
        score = round(bloc.get("tenderEvaluation", {}).get("totalScore", 0), 2)
        contractor_evals.append(
            {
                "name": bloc.get("contractorName", "Unknown"),
                "total_score": score,
                "evaluationSummary": summary,
            }
        )

    # scopes + weights
    scopes = [
        {
            "name": theme,
            "weight": sum(_coerce_weight_to_float(v) for v in weights.values()),
        }
        for theme, weights in evaluation_criteria.items()
    ]

    # final payload
    report = {
        "executive_summary": exec_summary,
        "contractorEvaluations": contractor_evals,
        "evaluationScopes": {
            "summary": (
                "Supplier submissions were assessed against the detailed "
                "technical evaluation criteria set out in the RFP."
            ),
            "scopes": scopes,
        },
    }

    logger.debug(
        "Technical report built with %d contractor rows", len(contractor_evals)
    )
    return report


async def tech_report_main(
    *,
    package_id: str | None = None,
    company_id: str | None = None,
    user_name: str | None = None,
    country_code: str = "USA",
    request_method: str | None = None,
    compute_reanalysis: bool = True,
    evaluation_criteria: Dict[str, Any] | None = None,
    **_,
) -> Dict[str, Any]:
    """
    Dispatcher for the technical-report tool.
    Uses job queue system for processing.
    """
    from utils.db.job_queue import JobType
    from tools.tech_rfp.job_queue_integration import (
        create_tech_rfp_job,
        get_job_status_response,
    )

    country_code = country_code or "USA"
    logger = pid_tool_logger(package_id=package_id, tool_name="tech_report")
    set_logger(logger)

    if not (package_id and company_id):
        return {"error": "packageId and companyId is required", "status": "error"}

    if request_method == "GET":
        return get_job_status_response(
            package_id=package_id,
            company_id=company_id,
            job_type=JobType.TECH_RFP_REPORT,
        )

    if request_method == "POST":
        if not compute_reanalysis:
            response = get_job_status_response(
                package_id=package_id,
                company_id=company_id,
                job_type=JobType.TECH_RFP_REPORT,
            )
            if response.get("status") == "completed":
                return response

        job_id = create_tech_rfp_job(
            job_type=JobType.TECH_RFP_REPORT,
            package_id=package_id,
            company_id=company_id,
            user_name=user_name,
            payload_fields={"evaluation_criteria": evaluation_criteria}
            if evaluation_criteria is not None
            else None,
            country_code=country_code,
        )

        logger.info(f"Created tech_report job {job_id} for project {package_id}")
        return {
            "status": "in progress",
            "message": f"Technical report job created for {package_id}",
            "job_id": job_id,
        }

    return {"error": f"Unsupported request method: {request_method}", "status": "error"}


# Extracted workflow function for job queue
async def _do_report_workflow(
    package_id: str,
    company_id: str,
    evaluation_criteria: Dict[str, Any] | str | bytes | bytearray | None = None,
    progress_callback=None,
) -> Dict[str, Any]:
    """
    Extracted workflow for technical report generation.
    Can be called directly by job processors.

    Args:
        package_id: Project identifier
        company_id: Company identifier
        progress_callback: Optional callback function(progress_dict) for progress updates

    Returns:
        Technical report dict
    """
    set_logger(pid_tool_logger(package_id, "tech_rfp_generate_report"))
    logger = get_logger()
    try:
        if progress_callback:
            progress_callback({"stage": "loading", "message": "Loading evaluation data"})
        from utils.db.job_queue import JobType

        analysis_bundle = get_latest_completed_artifact(
            package_id=package_id,
            company_id=company_id,
            job_type=JobType.TECH_RFP_ANALYSIS,
            artifact_type="result",
        )
        if not analysis_bundle or not analysis_bundle.get("data"):
            raise RuntimeError(
                "Required analysis result not found in DB job artifacts "
                "(job_type=tech_rfp_analysis, artifact_type=result)."
            )

        rfp_result_data = analysis_bundle["data"]
        evaluation_data = evaluation_criteria
        if evaluation_data is None:
            evaluation_data = (analysis_bundle.get("job", {}).get("payload") or {}).get(
                "evaluation_criteria"
            )

        if evaluation_data is None:
            raise RuntimeError(
                "Missing evaluation_criteria for report generation. "
                "Provide it in report job payload or ensure it exists in the source analysis job payload."
            )

        if isinstance(evaluation_data, (bytes, bytearray)):
            evaluation_data = json.loads(evaluation_data.decode("utf-8"))
        elif isinstance(evaluation_data, str):
            evaluation_data = json.loads(evaluation_data)
        elif not isinstance(evaluation_data, dict):
            raise ValueError(
                "Unexpected type for evaluation_criteria in report workflow: "
                f"{type(evaluation_data).__name__}"
            )

        if progress_callback:
            progress_callback(
                {"stage": "generating", "message": "Generating technical report"}
            )

        report = await build_technical_report(
            rfp_result_data=rfp_result_data,
            evaluation_criteria=evaluation_data,
        )

        logger.info("Technical report generated")
        return report
    except Exception as exc:
        logger.exception("Technical report workflow failed: %s", exc)
        raise


def main(*, package_id="system_check") -> bool:
    import asyncio
    import json
    from utils.core.log import pid_tool_logger, set_logger
    from tools.tech_rfp.tech_rfp_generate_report import build_technical_report

    print("TECH_RFP_GENERATE_REPORT TEST START")

    set_logger(
        pid_tool_logger(package_id="system_check", tool_name="tech_rfp_generate_report")
    )
    try:
        # Simulated RFP evaluation result (as if from tenderReport)
        rfp_result_data = {
            "result": {
                "tenderReport": [
                    {
                        "contractorName": "Alpha Tech",
                        "tenderEvaluation": {
                            "totalScore": 87.5,
                            "details": {
                                "Technical Capability": 35,
                                "Innovation": 30,
                                "Experience": 22.5,
                            },
                        },
                    },
                    {
                        "contractorName": "Beta Systems",
                        "tenderEvaluation": {
                            "totalScore": 78.0,
                            "details": {
                                "Technical Capability": 30,
                                "Innovation": 28,
                                "Experience": 20,
                            },
                        },
                    },
                ]
            }
        }

        # simulated criteria
        evaluation_criteria = {
            "Technical Capability": {"Expertise and Methodology": 40.0},
            "Innovation": {"Unique Features": 30.0},
            "Experience": {"Past Projects": 30.0},
        }

        # run report generation
        report = asyncio.run(
            build_technical_report(
                rfp_result_data=rfp_result_data,
                evaluation_criteria=evaluation_criteria,
            )
        )

        if not report or not isinstance(report, dict):
            raise RuntimeError("Generated report is empty or invalid.")

        print("TECH_RFP_GENERATE_REPORT OK")
        return True

    except Exception:
        print("TECH_RFP_GENERATE_REPORT ERROR")
        raise


if __name__ == "__main__":
    main()
